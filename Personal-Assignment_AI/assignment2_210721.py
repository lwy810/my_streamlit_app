# ex08.py
# https://www.hankyung.com/globalmarket/news-globalmarket
# í•œê²½ê¸€ë¡œë²Œ ë§ˆì¼“ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ íƒ€ì´í‹€ 10ê°œ / image urlì„ ì¶œë ¥í•˜ì‹œì˜¤.

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time
import csv
from datetime import datetime
from selenium.webdriver.common.action_chains import ActionChains

def crawl_global_it_news():
    """ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    
    # Chrome ì„¤ì •
    options = Options()
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    
    driver = webdriver.Chrome(options=options)
    
    try:
        print("ğŸš€ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
        
        # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸ í˜ì´ì§€ ì ‘ì†
        print("ğŸ“° ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì¤‘...")
        driver.get("https://new.land.naver.com/complexes?ms=37.3327729,127.1067075,15&a=APT:ABYG:JGC:PRE&e=RETAIL")
        time.sleep(3)

        # 2. ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘
        news_list = []                           
        # HTML ë¬¸ì„œì—ì„œ ëª¨ë“  <a> íƒœê·¸ë¥¼ ì„ íƒí•¨
        # news_1 = driver.find_elements(By.XPATH, "//a[contains(@class, '_cds_link') and contains(., 'ì¹˜í‚¨')]")
        # news_2 = driver.find_elements(By.XPATH, "//a[contains(@class, '_cds_link') and contains(., 'AI')]")
        # news_3 = driver.find_elements(By.XPATH, "//a[contains(@class, '_cds_link') and contains(., 'ê²½ì œ')]")

        # news_groups = [
        #     (news_1, "ì¹˜í‚¨"),
        #     (news_2, "AI"),
        #     (news_3, "ê²½ì œ")
        # ]


        news = driver.find_elements(By.XPATH, "//a[contains(@id,'COMPLEX')]/div/div/div/div/span[@class='price_default']")
        for i, news in enumerate(news):
                try:
                    title = news.text.strip()
                    # url = news.get_attribute('href')
                    print(f'({i+1}) {title}')
                    # print(f'({i+1}) {url}')
                    # print(f'({i+1}) {len(title)}')

                  
                    news_list.append({
                            'index': i + 1,
                            'title': title,
                            # 'link': url,
                            # 'keyword' : keyword,
                            'time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })
                    print(f"ğŸ“° [{len(news_list)}] {title[:50]}...")
                except:
                    continue
    
        return news_list




        # for news_group, keyword in news_groups :
        #     for i, news in enumerate(news_group):
        #         try:
        #             title = news.text.strip()
        #             url = news.get_attribute('href')
        #             print(f'({i+1}) {title}')
        #             # print(f'({i+1}) {url}')
        #             # print(f'({i+1}) {len(title)}')

        #             if title and url and len(title) > 10:  # ìœ íš¨í•œ ì œëª©ë§Œ
        #                 news_list.append({
        #                         'index': i + 1,
        #                         'title': title,
        #                         # 'link': url,
        #                         # 'keyword' : keyword,
        #                         'time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        #             })
        #             print(f"ğŸ“° [{len(news_list)}] {title[:50]}...")
        #         except:
        #             continue
    
        # return news_list
        
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {e}")
        return []
    finally:
        driver.quit()

def save_csv(news_list):
    """CSV íŒŒì¼ ì €ì¥"""
    if not news_list:
        print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    filename = f"it_news_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"
    
    with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
        # writer = csv.DictWriter(f, fieldnames=['index', 'keyword', 'title', 'link', 'time'])
        writer = csv.DictWriter(f, fieldnames=['index', 'title', 'time'])
        writer.writeheader()
        writer.writerows(news_list)
    
    print(f"âœ… ì €ì¥ì™„ë£Œ: {filename} ({len(news_list)}ê°œ)")

def main():
    # í¬ë¡¤ë§ ì‹¤í–‰
    news = crawl_global_it_news()
    
    # ê²°ê³¼ ì¶œë ¥
    if news:
        print(f"\nğŸ“Š ì´ {len(news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ì™„ë£Œ!")
        save_csv(news)
    else:
        print("âŒ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()